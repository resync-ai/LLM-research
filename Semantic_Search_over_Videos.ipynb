{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dce8c304",
      "metadata": {
        "id": "dce8c304"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/semantic-chunkers/blob/main/docs/01-video-chunking.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/aurelio-labs/semantic-chunkers/blob/main/docs/01-video-chunking.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3e65ca8-60c5-4936-bbbd-ccdb7fe3f0f4",
      "metadata": {
        "id": "b3e65ca8-60c5-4936-bbbd-ccdb7fe3f0f4"
      },
      "source": [
        "# Semantic search over chunked learning videos for ScormAI\n",
        "**Main idea:** Videos are a sequence of frames with a temporal component, so we try to identify the context between each scene or batch of frames.\n",
        "\n",
        "This concept is based on this source video: https://youtu.be/hsH9q_N02Gw?si=bQtS__SxPG3T2nXo\n",
        "\n",
        "**DISCLAIMER:** THIS IS JUST A PROOF OF CONCEPT! NO HATE, NO SELFISH FEEDBACK, NO FANCY CODE REVIEWS ACCEPTED!\n",
        "\n",
        "GOT A PROBLEM? OBVIOUSLY YOU DO...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KTTySx5PXSXr",
      "metadata": {
        "id": "KTTySx5PXSXr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Copyright (c) 2024 \"Imperator\" Radim Tvrdon. All rights reserved.\n",
        "\n",
        "This software and associated documentation files (the \"Software\") are the exclusive property of Mughla Chesky.\n",
        "Unauthorized copying, modification, distribution, or sale of the Software, in whole or in part, is strictly prohibited\n",
        "without the prior written permission of Imperator\n",
        "\n",
        "The Software is provided \"AS IS\", without warranty of any kind, express or implied, including but not limited to the warranties\n",
        "of merchantability, fitness for a particular purpose, or noninfringement. In no event shall the author or copyright holder\n",
        "be liable for any claim, damages, or other liability, whether in an action of contract, tort, or otherwise, arising from,\n",
        "out of, or in connection with the Software or the use or other dealings in the Software.\n",
        "\n",
        "For permission requests, please contact: radim@resync.cz\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fzm1Q_F_cajp",
      "metadata": {
        "id": "Fzm1Q_F_cajp"
      },
      "source": [
        "Install the dependencies. In this case we will be using the semantic chunkers and mainly the semantic-router lib and the OpenCV lib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8772039d",
      "metadata": {
        "id": "8772039d"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    \"semantic-chunkers[stats]\" \\\n",
        "    \"semantic-router[vision]==0.0.39\" \\\n",
        "    opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sKfr5seIb_W7",
      "metadata": {
        "id": "sKfr5seIb_W7"
      },
      "source": [
        "Init the cv2.vidcap library and load the source video for further processing. No worries about the source URL - talentwave.cz is the only accessible cloud instance where I can host any stuff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb4b9ef-6d56-473a-987b-fe5997480172",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eb4b9ef-6d56-473a-987b-fe5997480172",
        "outputId": "07878ad5-fae8-4406-bc94-b38f6fad9f43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1221"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "vidcap = cv2.VideoCapture (\"https://talentwave.cz/vids/pu_lesson.mp4\")\n",
        "\n",
        "frames = []\n",
        "success, image = vidcap.read()\n",
        "while success:\n",
        "    frames.append(image)\n",
        "    success, image = vidcap.read()\n",
        "len(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ekKxvZkfb5tP",
      "metadata": {
        "id": "ekKxvZkfb5tP"
      },
      "source": [
        "Let's load the frames, yo folks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4345eef5-2b9f-4925-a7be-b35e0be39492",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4345eef5-2b9f-4925-a7be-b35e0be39492",
        "outputId": "f105efa6-0374-4ed4-ff4e-511b64f6fe69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1221"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_frames = list(map(Image.fromarray, frames))\n",
        "len(image_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e054e963-2f02-4f8e-9fb9-6ec8172adf67",
      "metadata": {
        "id": "e054e963-2f02-4f8e-9fb9-6ec8172adf67"
      },
      "source": [
        "Now that we have the frames loaded, we can go ahead and use the `Chunker` functionality to create splits based on frame similarity\n",
        "\n",
        "First, lets initialise our ViT Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b98c7b-8e49-4e34-9c7e-2b95fb0b146c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8b98c7b-8e49-4e34-9c7e-2b95fb0b146c",
        "outputId": "0a835513-f1cf-4446-c0c5-fcbdd956e83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 'cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from semantic_router.encoders import VitEncoder\n",
        "from semantic_router.splitters.consecutive_sim import ConsecutiveSimSplitter\n",
        "\n",
        "device = (\n",
        "    \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "print(f\"Using '{device}'\")\n",
        "\n",
        "encoder = VitEncoder(device=device)\n",
        "\n",
        "splitter = ConsecutiveSimSplitter(encoder=encoder, score_threshold=0.5)\n",
        "splits = splitter(docs=image_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe1da2d-71ef-4e78-a56c-d96d1f55607e",
      "metadata": {
        "id": "cfe1da2d-71ef-4e78-a56c-d96d1f55607e"
      },
      "source": [
        "Now lets initialise our bad boy called Splitter.\n",
        "\n",
        "> Note: currently, we can only use `semantic_chunkers.chunkers.ConsecutiveChunker` for image content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import io\n",
        "\n",
        "b64_img_messages = []\n",
        "\n",
        "for split in splits:\n",
        "    # Get the middle frame from each split\n",
        "    middle_frame = split.docs[len(split.docs) // 2]\n",
        "\n",
        "    # Get image bytes\n",
        "    frame_bytes = io.BytesIO()\n",
        "    middle_frame.save(frame_bytes, format=\"JPEG\")\n",
        "\n",
        "    # Base64-encode the image bytes\n",
        "    b64_img = base64.b64encode(frame_bytes.getvalue()).decode(\"utf-8\")\n",
        "    b64_img_messages.append(\n",
        "        {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "                \"url\": f\"data:image/jpeg;base64,{b64_img}\"\n",
        "            }\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "Duj4GPDDtQzd"
      },
      "id": "Duj4GPDDtQzd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "eFv0TM_X5p1F"
      },
      "id": "eFv0TM_X5p1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rxr8iszG6fe-"
      },
      "id": "rxr8iszG6fe-"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "print(os.getenv('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-ekhTe05Jzq",
        "outputId": "ecf0356b-b62d-464f-fb86-431a09368eea"
      },
      "id": "V-ekhTe05Jzq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  organization='org-3jyWJoqwwQE9aOIIjR5wEfZk',\n",
        "  project='$PROJECT_ID',\n",
        ")"
      ],
      "metadata": {
        "id": "s8cTRFLl5dMc"
      },
      "id": "s8cTRFLl5dMc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "client = openai.Client()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"The following series of images are sampled frames from a video, in chronological order. What's happening in the video?\"\n",
        "            },\n",
        "            *b64_img_messages,\n",
        "        ]\n",
        "    }],\n",
        "    stream=False,\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "UJ0BcNWg5bXN",
        "outputId": "ab4801c4-749c-45a8-c811-1363666eb8dd"
      },
      "id": "UJ0BcNWg5bXN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'b64_img_messages' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fca595347a73>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"The following series of images are sampled frames from a video, in chronological order. What's happening in the video?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             },\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;34m*\u001b[0m\u001b[0mb64_img_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         ]\n\u001b[1;32m     16\u001b[0m     }],\n",
            "\u001b[0;31mNameError\u001b[0m: name 'b64_img_messages' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8131742",
      "metadata": {
        "id": "d8131742"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install moviepy\n",
        "!pip install ffmpeg-python\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "5fYkMz5l8LuK"
      },
      "id": "5fYkMz5l8LuK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "video_url = \"https://talentwave.cz/vids/pu_lesson.mp4\"\n",
        "\n",
        "video_filename = os.path.basename(video_url)\n",
        "\n",
        "response = requests.get(video_url, stream=True)\n",
        "\n",
        "with open(video_filename, 'wb') as video_file:\n",
        "    for chunk in response.iter_content(chunk_size=1024):\n",
        "        if chunk:\n",
        "            video_file.write(chunk)\n",
        "\n",
        "print(f\"Video bylo staženo a uloženo jako {video_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3py-K4FdGFrT",
        "outputId": "78fd1589-60ea-41f9-e2a4-f62041bf9629"
      },
      "id": "3py-K4FdGFrT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video bylo staženo a uloženo jako pu_lesson.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import *\n",
        "\n",
        "video_filename = video_filename\n",
        "video = VideoFileClip(video_filename)\n",
        "audio = video.audio\n",
        "audio_filename = os.path.splitext(video_filename)[0] + \"_ext-audio.mp3\"\n",
        "audio.write_audiofile(audio_filename)\n",
        "\n",
        "print(f\"Audio bylo uloženo jako {audio_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iO4yF38lILFm",
        "outputId": "ec518a3c-424c-49c5-9c60-ec03e60018fc"
      },
      "id": "iO4yF38lILFm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'video_filename' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4126bbd724cc>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meditor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvideo_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'video_filename' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import librosa"
      ],
      "metadata": {
        "id": "pDkzN_p3KZBG"
      },
      "id": "pDkzN_p3KZBG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_audio_files(path, extension=\".mp3\"):\n",
        "    audio_files = []\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for f in files:\n",
        "            if f.endswith(extension):\n",
        "                audio_files.append(os.path.join(root, f))\n",
        "\n",
        "    return audio_files"
      ],
      "metadata": {
        "id": "nGEb2GIaKGmR"
      },
      "id": "nGEb2GIaKGmR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_audio(filename, segment_length: int, output_dir):\n",
        "    \"\"\"segment lenght is in seconds\"\"\"\n",
        "\n",
        "    print(f\"Chunking audio to {segment_length} second segments...\")\n",
        "\n",
        "    if not os.path.isdir(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    audio, sr = librosa.load(filename, sr=44100)\n",
        "\n",
        "    duration = librosa.get_duration(y=audio, sr=sr)\n",
        "    num_segments = int(duration / segment_length) + 1\n",
        "\n",
        "    print(f\"Chunking {num_segments} chunks...\")\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        start = i * segment_length * sr\n",
        "        end = (i + 1) * segment_length * sr\n",
        "        segment = audio[start:end]\n",
        "        sf.write(os.path.join(output_dir, f\"segment_{i}.mp3\"), segment, sr)\n",
        "\n",
        "    chunked_audio_files = find_audio_files(output_dir)\n",
        "    return sorted(chunked_audio_files)"
      ],
      "metadata": {
        "id": "Q8VXmiPHKXel"
      },
      "id": "Q8VXmiPHKXel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(audio_files: list, output_file=None, model=\"whisper-1\") -> list:\n",
        "\n",
        "    print(\"converting audio to text...\")\n",
        "\n",
        "    transcripts = []\n",
        "    for audio_file in audio_files:\n",
        "        audio = open(audio_file, \"rb\")\n",
        "        response = openai.Audio.transcribe(model, audio)\n",
        "        transcripts.append(response[\"text\"])\n",
        "\n",
        "    if output_file is not None:\n",
        "        # save all transcripts to a .txt file\n",
        "        with open(output_file, \"w\") as file:\n",
        "            for transcript in transcripts:\n",
        "                file.write(transcript + \"\\n\")\n",
        "\n",
        "    return transcripts"
      ],
      "metadata": {
        "id": "PzZqA8zsKqFC"
      },
      "id": "PzZqA8zsKqFC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"small\")\n",
        "\n",
        "option = whisper.DecodingOptions(language='cs', fp16=False)\n",
        "result = model.transcribe('pu_lesson.mp4', **option.__dict__)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "mHfbQTIOLKfE",
        "outputId": "3ce42de8-c01b-4890-97db-be3c3112d7fc"
      },
      "id": "mHfbQTIOLKfE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'whisper'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-07353c6bc55c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecodingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whisper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def prepis_textu(client, input_text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": input_text\n",
        "                    },\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    result = response.choices[0].message.content\n",
        "    print(result)\n",
        "\n",
        "# Použití funkce\n",
        "client = openai.Client()\n",
        "text_to_rewrite = \"In the following text, letters are missing within words. At the same time, the text is difficult to read for the average user. Rewrite this text so that it is fluent and easy to read.\"\n",
        "prepis_textu(client, text_to_rewrite)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-p3GLyTOE7t",
        "outputId": "f58f81a7-a099-4995-e43a-10ac4c791b5a"
      },
      "id": "3-p3GLyTOE7t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolutely, I'd be happy to help! Please provide the text you would like me to rewrite.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}